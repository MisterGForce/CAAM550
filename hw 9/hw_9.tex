\documentclass{article} % \documentclass{} is the first command in any LaTeX code.  It is used to define what kind of document you are creating such as an article or a book, and begins the document preamble

\usepackage{amsmath} % \usepackage is a command that allows you to add functionality to your LaTeX code
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{siunitx}
\setlength{\parindent}{0pt}

\begin{document} % All begin commands must be paired with an end command somewhere
\textbf{Michael Goforth} \\
\textbf{CAAM 550} \\
\textbf{HW 9} \\
\textbf{11/05/2021} \\ 

\textbf{Problem 1} \\
See Jupyter notebook for code and results.
\\


\textbf{Problem 2} \\

\textbf{part a} \\
Let $h_i = x_{i+1} - x_i$ and
\begin{equation*}
\hat{x_i} = \frac{x - x_i}{h_i}
\end{equation*}
Then the Hermite polynomials are
\begin{align*}
H_0(\hat{x}) &= (1 - \hat{x})^2(1+2\hat{x}) \\
H_1(\hat{x}) &= \hat{x}^2(3-2\hat{x}) \\
h_0(\hat{x}) &= \hat{x}(1-\hat{x})^2 \\
h_1(\hat{x}) &= \hat{x}^2(\hat{x}-1) \\
H_0'(\hat{x}) &= \frac{1}{h_i}(6\hat{x}^2 - 6\hat{x})dx \\
H_1'(\hat{x}) &= \frac{1}{h_i}(-6\hat{x}^2 - 6\hat{x})dx \\
h_0'(\hat{x}) &= \frac{1}{h_i}(-3\hat{x}^2 - 4\hat{x} + 1)dx \\
h_1'(\hat{x}) &= \frac{1}{h_i}(3\hat{x}^2 - 2\hat{x})dx \\
\end{align*}
The interpolating polynomials can then be constructed as
\begin{equation*}
P_i(\hat{x}) = c_1 H_0(\hat{x}) + c_2 H_1(\hat{x}) + c_3 h_0 (\hat{x}) + c_4 h_1 (\hat{x}),\ x \in [x_i, x_i+1]
\end{equation*}
In order for continuity of both the function and its derivative 
\begin{align*}
P_i(x_i) &= f_i \\
P_i(x_i+1) &= f_{i+1} \\
P_i'(x_i) &= f_i' \\
P_i'(x_i+1) &= f'_{i+1} \\ 
\end{align*}
Then 
\begin{align*}
P_i(x_i) = c_1 H_0(0) + c_2 H_1 (0) + c_3 h_0(0) + c_4 h_1(0) = c_1 = f_i \\
P_i(x_{i+1}) = c_1 H_0(1) + c_2 H_1 (1) + c_3 h_0(1) + c_4 h_1(1) = c_2 = f_{i+1} \\
P_i'(x_i) = c_1 H'_0(0) + c_2 H'_1 (0) + c_3 h'_0(0) + c_4 h'_1(0) = \frac{1}{h_i} c_3 = f'_i \\
P_i'(x_{i+1}) = c_1 H'_0(1) + c_2 H'_1 (1) + c_3 h'_0(1) + c_4 h'_1(1) = \frac{1}{h_i} c_4 = f'_{i+1} \\
\end{align*}
Combining the above equations gives the transformation
\begin{multline*}
P_i(x) = f_i H_0(\frac{x - x_i}{x_{i+1} - x_i}) + f_{i+1} H_1 (\frac{x - x_i}{x_{i+1} - x_i}) \\
+ (x_{i+1} - x_i) f'_i h_0(\frac{x - x_i}{x_{i+1} - x_i}) + (x_{i+1} - x_i) f'_{i+1} h_1(\frac{x - x_i}{x_{i+1} - x_i})
\end{multline*}
\\

\textbf{part b} \\
See Jupyter notebook for code and results.
\\


\textbf{Problem 3} \\

\textbf{part a} \\
\begin{equation*}
B_n^i(x) = \begin{pmatrix} n \\ i \end{pmatrix} (1-x)^{n-i}x^i
\end{equation*}
where 
\begin{equation*}
\begin{pmatrix} n \\ i \end{pmatrix} = \frac{n!}{i!(n-i)!}
\end{equation*}
for $0 \leq i \leq n$.  On the interval [0, 1], $1 - x$, $n-i$, and $x$ are all greater than or equal to zero, so the Bernstein polynomials also are greater than or equal to zero. \\
From the binomial theorem:
\begin{equation*}
(x+y)^n = \sum_{i=0}^n \begin{pmatrix} n \\ i \end{pmatrix} x^i y^{n-i}
\end{equation*}
if $y = 1 - x$ then it is easy to see from the binomial theorem that the sum of the Bernstein Polynomials is
\begin{equation*}
\sum_{i=0}^n B_i^n(x)=\sum_{i=0}^n \begin{pmatrix} n \\ i \end{pmatrix} x^i (x-1)^{n-i} = (x + 1 - x)^n = 1
\end{equation*}
\\

\textbf{part b} \\
\begin{align*}
B_i^n(x) &= \begin{pmatrix} n \\ i \end{pmatrix} (1-x)^{n-i}x^i \\
xB_{i-1}^{n-1}(x) &= \begin{pmatrix} n-1 \\ i - 1 \end{pmatrix} (1-x)^{n-i}x^{i} \\
(1-x)B_{i}^{n-1}(x) &= \begin{pmatrix} n-1 \\ i \end{pmatrix} (1-x)^{n-i}x^{i} \\
\begin{pmatrix} n-1 \\ i - 1 \end{pmatrix} &= \frac{(n-1)!}{(i-1)!(n-i)!} = \frac{i}{n} \frac{n!}{i!(n-i)!}\\
\begin{pmatrix} n-1 \\ i \end{pmatrix} &= \frac{(n-1)!}{i!(n-i-1)!} = \frac{n-i}{n} \frac{n!}{i!(n-i)!} \\
\begin{pmatrix} n-1 \\ i \end{pmatrix} + \begin{pmatrix} n-1 \\ i \end{pmatrix} &= \frac{n!}{i!(n-i)!} = \begin{pmatrix} n \\ i \end{pmatrix}
\end{align*}
So then
\begin{equation*}
xB_{n-1}^{i-1}(x) + (1-x)B_{n-1}^{i}(x) = B_n^i(x) 
\end{equation*}
\\
\textbf{part c} \\
Let 
\begin{equation*}
B(x) = \sum_{i=0}^n b_i B_i^n
\end{equation*}
be a polynomial of degree $n$ written in the Bernstein basis with coefficients $b_0, b_1, \hdots, b_n$.  Let $T \in \mathbb{R}^{(n+1)\times(n+1)}$ be the transformation matrix that converts the coefficients of the Bernstein basis into coefficients of the monomial basis.  Then
\begin{equation*}
T \begin{bmatrix} b_0 \\ b_1 \\ \vdots \\ b_n \end{bmatrix} = \begin{pmatrix} c_0 \\ c_1 \\ \vdots \\ c_n \end{pmatrix}
\end{equation*}
where $c_0, c_1, \hdots, c_n$ are the coefficients in the monomial basis such that
\begin{equation*}
B(x) = \sum_{i=0}^n b_i B_i^n = \sum_{i=0}^n c_i x^i
\end{equation*}
Consider the $j$-th term in the Bernstein polynomial sum above.
\begin{equation*}
c_j B_j^n = \begin{pmatrix} n \\ j \end{pmatrix} (1 - x)^{n-j} x^j \\
\end{equation*}
Using the binomial theorem to expand $(1-x)^{n-j}$ gives
\begin{align*}
c_j B_j^n &= c_j \begin{pmatrix} n \\ j \end{pmatrix} x^j  \sum_{l=0}^{n-j} \begin{pmatrix} 
n-j \\ l \end{pmatrix} (-x)^{n-j-l} \\
c_j B_j^n &= c_j \begin{pmatrix} n \\ j \end{pmatrix} \sum_{l=0}^{n-j} \begin{pmatrix} 
n-j \\ l \end{pmatrix} (-1)^{n-j-l} x^{n-l} \\
\end{align*}
which converts $B_j^n$ into a sum of the monomial basis vectors where the $l$-th term in the sum corresponds to the $(n-l)$-th monomial.  Then 
\begin{equation*} 
c_i = \sum_{j=0}^n c_j \begin{pmatrix} n \\ j \end{pmatrix} \begin{pmatrix} 
n-j \\ n-i \end{pmatrix} (-1)^{i-j} 
\end{equation*}
From this sum it can be seen that the elements in the transformation matrix T can be expressed as
\begin{align*}
T_{i, j} &= \begin{pmatrix} n \\ j \end{pmatrix} \begin{pmatrix} n-j \\ n - i \end{pmatrix} (-1)^{i-j}\ \text{if}\ j \leq i \\
T_{i, j} &= 0\ \text{if}\ j > i
\end{align*}
where indices $i, j$ range from 0 to $n$. \\
Further
\begin{align*}
\begin{pmatrix} n \\ j \end{pmatrix} 
\begin{pmatrix} n - j \\ n - 1 \end{pmatrix} &= \frac{n!}{j!(n-j)!} 
\frac{(n-j)!}{(n-i)!(n-j-(n-i))!} \\
\begin{pmatrix} n \\ j \end{pmatrix} 
\begin{pmatrix} n - j \\ n - 1 \end{pmatrix} &= \frac{n!}{j!(n-i)!(i-j)!}
\end{align*}
For example, for $n=3$:
\begin{align*}
T &= \begin{bmatrix} T_{0,0} & T_{0, 1} & T_{0,2} & T_{0,3} \\
T_{1,0} & T_{1, 1} & T_{1,2} & T_{1,3} \\
T_{2,0} & T_{2, 1} & T_{2,2} & T_{2,3} \\
T_{3,0} & T_{3, 1} & T_{3,2} & T_{3,3} \end{bmatrix} \\
T &= \begin{bmatrix} \frac{n!}{0!(n-0)!(0-0)!}(-1)^{0-0} & 0 & 0 & 0 \\
\frac{3!}{0!(3-1)!(1-0)!}(-1)^{1-0} & \frac{3!}{1!(3-1)!(1-1)!}(-1)^{1-1} & 0 & 0 \\
\frac{3!}{0!(3-2)!(2-0)!}(-1)^{2-0} & \frac{3!}{1!(3-2)!(2-1)!}(-1)^{2-1} &
\frac{3!}{2!(3-2)!(2-2)!}(-1)^{2-2} & 0 \\
\frac{3!}{0!(3-3)!(3-0)!}(-1)^{3-0} & \frac{3!}{1!(3-3)!(3-1)!}(-1)^{3-1} & 
\frac{3!}{2!(3-3)!(3-2)!}(-1)^{3-2} & \frac{3!}{3!(3-3)!(3-3)!}(-1)^{3-3}
\end{bmatrix} \\
T &= \begin{bmatrix} 1 & 0 & 0 & 0 \\
-3 & 3 & 0 & 0 \\
3 & -6 & 3 & 0 \\
-1 & 3 & -3 & 1 \end{bmatrix}
\end{align*}
\\


\textbf{Problem 4} \\
\textbf{part i} \\
See Jupyter notebook for code and results.  
\\

\textbf{part ii} \\
See Jupyter notebook for code and results.  
\\

\textbf{part iv (iii?)} \\
See Jupyter notebook for code and results. 
\\
\\

\textbf{Problem 5} \\

\textbf{part i} \\
This method leads to a system of equations that can be represented as:
\begin{equation*}
\begin{bmatrix} 1 & 1 & 1 & \hdots & 1 \\
x_0 & x_1 & x_2 & \hdots & x_n \\
x_0^2 & x_1^2 & x_2^2 & \hdots & x_n^2 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
x_0^n & x_1^n & x_2^n & \hdots & x_n^n \\
\end{bmatrix} \begin{bmatrix} w_0 \\ w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}
= \begin{bmatrix} b - a \\ 1/2 (b^2 - a^2) \\ 1/3 (b^3  - a^3) \\ \vdots \\ \frac{1}{n+1}(b^{n+1} - a^{n+1} \end{bmatrix}
\end{equation*}
The first matrix is the transpose of the Vandermonde matrix, and similar to the Vandermonde matrix, this matrix is ill-conditioned and prone to floating point errors.  As a result computing quadrature weights with this method could lead to large errors.
\\

\textbf{part ii} \\
See Jupyter notebook for code and results. \\
For $n=10$ and $n=15$ negative weights occur.  This is not surprising due to the poor conditioning of the problem.  $n=10$ and $n=15$ also show large error (see part iv results for closer approximations).
\\

\textbf{part iii} \\
Based on our discussion of polynomial interpolation, Chebyshev Points will give better quadrature than equidistant points.
\begin{equation*}
x_i = \frac{1}{2}(a+b) + \frac{1}{2}(b-a)\cos(\frac{(2i-1)\pi}{2n})
\end{equation*}
See Jupyter notebook for code and results. \\
There are no longer negative weights used for approximations when $n=10$ and $n=15$.  Results are also more consistent.
\\

\textbf{part iv} \\
See Jupyter notebook for code and results.
\\



\end{document} % This is the end of the document


